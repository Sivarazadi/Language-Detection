{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading English-Kurdish dataset\n",
    "Kurdish_df = pd.DataFrame(pd.read_csv('/Users/sivarazadi/Programming_Projects/Language_Detection/wordlist2.csv'))\n",
    "Kurdish_df = Kurdish_df[12:] #deleting multiple 'A' translations to make things simpler \n",
    "print(Kurdish_df)\n",
    "\n",
    "# loading Languages dataset\n",
    "Languages_df = pd.DataFrame(pd.read_csv('/Users/sivarazadi/Programming_Projects/Language_Detection/Language Detection.csv'))\n",
    "print(Languages_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|       | wordId  |english   |kurdish  |english_length  |kurdish_length|\n",
    "| :---        |    :----:   |          ---: |    :----:   |          ---: |         ---: |\n",
    "| 12          |  13         |    abandon    |    berdan    |         7.0    |         6.0|\n",
    "|  13          |  14         |    abate      |    kêmkirin   |          5.0   |          8.0|\n",
    "|  14          |  15         |    abide      |    ragirtin    |         5.0    |         8.0|\n",
    "|  15          |  16         |    about      |    der barê     |        5.0     |        8.0|\n",
    "|  16          |  17         |    above      |     jor          |   5.0          |   3.0|\n",
    "|  ...         |  ...        |      ...           |     ...      |       ...      |       ...|\n",
    "|  4324        |  4330       |     your           |  yên we       |      4.0       |     6.0|\n",
    "|  4325        |  4331       |    youth           |  ciwanî        |     5.0        |     6.0|\n",
    "|  4326        |  4332       |    youth           | xortanî         |    5.0         |    7.0|\n",
    "| 4327        |  4333       |     zinc           |   tûtya          |   4.0          |   5.0|\n",
    "|  4328        |  4334       |     zone           |   cîwar           |  4.0           |  5.0|\n",
    "\n",
    "  [4317 rows x 5 columns] \n",
    "\n",
    "\n",
    "|       | Text | Language|\n",
    "| :---        |    :----:   |          ---: |                                          \n",
    "|0      | Nature, in the broadest sense, is the natural... | English|\n",
    "|1      |\"Nature\" can refer to the phenomena of the phy...  |English|\n",
    "|2      |The study of nature is a large, if not the onl...  |English|\n",
    "|3      |Although humans are part of nature, human acti... | English|\n",
    "|4      |[1] The word nature is borrowed from the Old F... | English|\n",
    "|...    |                                              ...  |    ...|\n",
    "|10332  |ನಿಮ್ಮ ತಪ್ಪು ಏನು ಬಂದಿದೆಯೆಂದರೆ ಆ ದಿನದಿಂದ ನಿಮಗೆ ಒ...  |Kannada|\n",
    "|10333  |ನಾರ್ಸಿಸಾ ತಾನು ಮೊದಲಿಗೆ ಹೆಣಗಾಡುತ್ತಿದ್ದ ಮಾರ್ಗಗಳನ್...  |Kannada|\n",
    "|10334  |ಹೇಗೆ ' ನಾರ್ಸಿಸಿಸಮ್ ಈಗ ಮರಿಯನ್ ಅವರಿಗೆ ಸಂಭವಿಸಿದ ಎ...|  Kannada|\n",
    "|10335  |ಅವಳು ಈಗ ಹೆಚ್ಚು ಚಿನ್ನದ ಬ್ರೆಡ್ ಬಯಸುವುದಿಲ್ಲ ಎಂದು ...  |Kannada|\n",
    "|10336  |ಟೆರ್ರಿ ನೀವು ನಿಜವಾಗಿಯೂ ಆ ದೇವದೂತನಂತೆ ಸ್ವಲ್ಪ ಕಾಣು...  | Kannada|\n",
    "\n",
    "[10337 rows x 2 columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning English-Kurdish dataset and added it to Languages dataset\n",
    "Kurdish_df = Kurdish_df[['kurdish']]\n",
    "Kurdish_df['Language'] = 'Kurdish'\n",
    "Kurdish_df = Kurdish_df.rename(columns={'kurdish':'Text'})\n",
    "\n",
    "Languages_df = pd.concat([Languages_df, Kurdish_df])\n",
    "print(Languages_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|       | Text | Language|\n",
    "| :---        |    :----:   |          ---: |                                          \n",
    "|0     | Nature, in the broadest sense, is the natural...  |English|\n",
    "|1     |\"Nature\" can refer to the phenomena of the phy...  |English|\n",
    "|2     |The study of nature is a large, if not the onl... | English|\n",
    "3     |Although humans are part of nature, human acti...  |English|\n",
    "|4     |[1] The word nature is borrowed from the Old F...  |English|\n",
    "|...    |                                             ...  |    ...|\n",
    "|4324    |                                         yên we  |Kurdish|\n",
    "|4325     |                                        ciwanî  |Kurdish|\n",
    "|4326      |                                      xortanî  |Kurdish|\n",
    "|4327       |                                       tûtya  |Kurdish|\n",
    "|4328        |                                      cîwar  |Kurdish|\n",
    "\n",
    "[14654 rows x 2 columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled dataset and split into X and y for trainging\n",
    "shuffled_Language_df = Languages_df.sample(frac=1)\n",
    "X = shuffled_Language_df['Text']\n",
    "y = shuffled_Language_df['Language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at unique label values\n",
    "print(y.unique())\n",
    "print(len(y.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['English' 'Greek' 'Turkish' 'Spanish' 'Arabic' 'Dutch' 'Kurdish' <br/>\n",
    " 'Portugeese' 'Sweedish' 'Kannada' 'Danish' 'French' 'Russian' 'Malayalam' <br/>\n",
    " 'Tamil' 'Italian' 'German' 'Hindi'] <br/>\n",
    " \n",
    "18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding label data for training \n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting rid of special characters and upper-case characters\n",
    "X_list = []\n",
    "for text in X:\n",
    "    text = re.sub(r'[!@#$(),\\n\"%^*?\\:;~`0-9]', ' ', text)\n",
    "    text = re.sub(r'[[]]', ' ', text)\n",
    "    text = text.lower()\n",
    "    X_list.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing and splitting data\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(X_list).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure label data is categorical\n",
    "y_train = to_categorical(y_train, 18)\n",
    "y_test = to_categorical(y_test, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_shape=( None,41610)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(18, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss=categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: \"sequential\" <br/>\n",
    "|      Layer (type) | Output Shape  | Param #|\n",
    "| :---        |    :----   |          ---: |                                          \n",
    "| dense (Dense)     |          (None, None, 128)    |     5326208   |                                                            \n",
    "| dense_1 (Dense) |            (None, None, 128)    |     16512     |                                                                \n",
    "| dense_2 (Dense)  |           (None, None, 64)     |     8256      |                                                                \n",
    "| dense_3 (Dense)   |          (None, None, 18)     |     1170      |\n",
    "|Total params: 5,352,146|\n",
    "|Trainable params: 5,352,146|\n",
    "|Non-trainable params: 0|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint for best training epoch\n",
    "checkpoint_filepath = \"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "callbacks_list = [model_checkpoint_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with validation\n",
    "history = model.fit(x=X_train, y=y_train, epochs=5, validation_data=[X_test, y_test], callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/5 <br/>\n",
    "\n",
    "454/454 [==============================] - 15s 32ms/step - loss: 0.5743 - accuracy: 0.8711 - val_loss: 0.1396 - val_accuracy: 0.9592 <br/>\n",
    "\n",
    "Epoch 2/5 <br/>\n",
    "\n",
    "454/454 [==============================] - 14s 30ms/step - loss: 0.0518 - accuracy: 0.9887 - val_loss: 0.1892 - val_accuracy: 0.9728 <br/>\n",
    "\n",
    "Epoch 3/5 <br/>\n",
    "\n",
    "454/454 [==============================] - 14s 31ms/step - loss: 0.0143 - accuracy: 0.9973 - val_loss: 0.1948 - val_accuracy: 0.9796 <br/>\n",
    "\n",
    "Epoch 4/5 <br/>\n",
    "\n",
    "454/454 [==============================] - 13s 30ms/step - loss: 0.0105 - accuracy: 0.9978 - val_loss: 0.2121 - val_accuracy: 0.9796 <br/>\n",
    "\n",
    "Epoch 5/5 <br/>\n",
    "\n",
    "454/454 [==============================] - 13s 29ms/step - loss: 0.0086 - accuracy: 0.9977 - val_loss: 0.3963 - val_accuracy: 0.8980 <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting acc\n",
    "\n",
    "def plot_accuracy(acc, val_acc):\n",
    "    plt.figure()\n",
    "    plt.plot(acc)\n",
    "    plt.plot(val_acc)\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show() \n",
    "\n",
    "plot_accuracy(history.history['accuracy'], history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Accuracy_Plot.png\" style=\"width:600px;height:400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = max(history.history['val_accuracy'])\n",
    "print( 'best val_accuracy score: ' + str(best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best val_accuracy score: 0.9795918464660645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath='weights-improvement-03-0.98.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "     x = vectorizer.transform([text]).toarray() # converting text to bag of words model (Vector)\n",
    "     lang = model.predict(x) # predicting the language\n",
    "     lang = encoder.inverse_transform(lang) # finding the language corresponding the the predicted value\n",
    "     print(\"The langauge is in\",lang[0]) # printing the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The langauge is in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('welgemanierd heeft het woord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The langauge is in Dutch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
